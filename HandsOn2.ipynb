{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the first handson!!!\n",
    "- In this handson you will be building an deep neural network network for multiclass classification using softmax regression.\n",
    "- You will also be implementing minibatch gradient descent to train you network\n",
    "- Follow the instruction provided for cell to write the code in each cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- The data is provided as file named 'data.csv'.\n",
    "- The data has two features feature1 and feature2 and one targer variable which is a binary value\n",
    "- Using pandas read the csv file and assign the resulting dataframe to variable 'data'   \n",
    "- for example if file name is 'xyz.csv' read file as **pd.read_csv('xyz.csv')**\n",
    "- Packages to import: **pandas** (to read csv file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###start code here\n",
    "                 #import pandas \n",
    "data = \n",
    "### end code(aprox 2 lines)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Extract feature1 and feature2 values from dataframe 'df' and assign it to variable 'X'\n",
    "- Extract target variable 'traget' and assign it to variable 'y'.  \n",
    "Hint:\n",
    " - Use .values to exract values from dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start Code\n",
    "X =                                          #Extract feature1 and feature2 values\n",
    "Y =                                          #Extract target values\n",
    "#End code\n",
    "print(\"target labels: \", list(set(Y)))\n",
    "\n",
    "assert X.shape == (2000, 2)\n",
    "assert Y.shape == (2000, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are four target lables mapped as 0,1,2 and 3\n",
    "- Run the below cell to visualize the data in x-y plane. (visualization code has been written for you)\n",
    "- The blue spots corresponds to target value 0, green corresponds to 1, red corresponds to 2 and yellow to 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors\n",
    "colors=['blue','green','red', 'yellow']\n",
    "cmap = matplotlib.colors.ListedColormap(colors)\n",
    "#Plot the figure\n",
    "plt.figure()\n",
    "plt.title('Non-linearly separable classes')\n",
    "plt.scatter(X[:,0], X[:,1], c=Y,\n",
    "           marker= 'o', s=50,cmap=cmap,alpha = 0.5 )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import tensorflow package as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "                 #import tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- perform one hot encoding on target Y. Use tensorflow's one_hot() function, make sure mapping is done column-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(set(Y))\n",
    "#Start Code\n",
    "depth =                    #number of unique labels\n",
    "#End code\n",
    "print(\"labels: \", labels)\n",
    "with tf.Session() as sess:\n",
    "###start code here\n",
    "    YtrainOneHot = \n",
    "    Y_onehot = \n",
    "###End code\n",
    "assert Y_onehot.shape == (4, 2000)\n",
    "\n",
    "print(\"\\nfirst five rows of target Y:\\n\", Y[:5])\n",
    "print(\"\\nfirst five rows of target Y_onehot:\\n\", Y_onehot[:,:5])\n",
    "print(\"X dimension:{} ,Y_onehot dimension:{}\".format(X.shape, Y_onehot.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In order to feed the network the input has to be of **shape (number of features, number of samples)** and target should be of shape **(1, number of samples)**\n",
    "- Transpose X and assign it to variable 'X_data'\n",
    "- target Y_onehot is already in shape, so just assign Y to variable Y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Start code\n",
    "X_data = \n",
    "Y_data = \n",
    "##End code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the network dimension to have two input features, two hidden layers with 25 nodes each, 4 output nodes at final layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Start code\n",
    "layer_dims = \n",
    "#End code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function named placeholders to return two placeholders one for input data as A_0 and one for output data as Y.\n",
    "- Set the datatype of placeholders as float64\n",
    "- parameters - num_features\n",
    "- Returns - A_0 with shape (num_feature, None) and Y with shape(1,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def placeholders(num_features, num_classes):\n",
    "    A_0 = tf.placeholder(dtype = tf.float64, shape = ([num_features,None]))\n",
    "    Y = tf.placeholder(dtype = tf.float64, shape = ([num_classes,None]))\n",
    "    return A_0,Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function named initialize_parameters_deep() to initialize weights and bias for each layer\n",
    "- Use tf.get_variable to initialise weights and bias, set datatype as float64\n",
    "- Make sure you are using xavier initialization for weigths and initialize bias to zeros\n",
    "- Parameters - layer_dims\n",
    "- Returns - dictionary of weights and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    tf.set_random_seed(1)\n",
    "    L = len(layer_dims)\n",
    "    parameters = {}\n",
    "    for l in range(1,L):\n",
    "        parameters['W' + str(l)] = tf.get_variable(\"W\" + str(l), shape=[layer_dims[l], layer_dims[l-1]], dtype = tf.float64,\n",
    "                                   initializer=tf.contrib.layers.xavier_initializer())\n",
    "        parameters['b' + str(l)] = tf.get_variable(\"b\"+ str(l), shape = [layer_dims[l], 1], dtype= tf.float64, initializer= tf.zeros_initializer() )\n",
    "    return parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functon named linear_forward_prop() to define forward propagation for a given layer.\n",
    "- parameters: A_prev(output from previous layer), W(weigth matrix of current layer), b(bias vector for current layer),activation(type of activation to be used for out of current layer)  \n",
    "- returns: A(output from the current layer)\n",
    "- Use relu activation for hidden layers and for final output layer return the output unactivated i.e if activation is 'softmax'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_forward_prop(A_prev,W,b, activation):\n",
    "    #Start code here \n",
    "    Z =       \n",
    "    if activation == \"softmax\":\n",
    "        A = \n",
    "    elif activation == \"relu\":\n",
    "        A = \n",
    "    return A\n",
    "   #End code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "forward propagation for entire network is defined for you as l_layer_forward(). We are not using any regularization technique here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def l_layer_forwardProp(A_0, parameters):\n",
    "    A = A_0\n",
    "    L = len(parameters)//2\n",
    "    for l in range(1,L):\n",
    "        A_prev = A\n",
    "        A = linear_forward_prop(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], \"relu\")\n",
    "    A = linear_forward_prop(A, parameters['W' + str(L)], parameters['b' + str(L)], \"softmax\" )\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Define cost function as final_cost()\n",
    "- The logits and target value has to be in shape (number of samples, number of class) for      \n",
    "softmax_cross_entropy_with_logits() function to calculate cost\n",
    "- since Z_final and Y are tensoflow object we are using tf.transpose() before feeding  softmax_cross_entropy_with_logits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def final_cost(Z_final, Y ):\n",
    "    logits = tf.transpose(Z_final)\n",
    "    labels = tf.transpose(Y)\n",
    "    ###Start code\n",
    "    cost =                    #use tensorflow's softmax_cross_entropy to calculate cost\n",
    "    ###End code\n",
    "    return tf.reduce_mean(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the function to generate mini-batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def random_samples_minibatch(X, Y, batch_size, seed = 1):\n",
    "    np.random.seed(seed)\n",
    "    m = X.shape[1]                                 #number of samples\n",
    "    num_batches = int(m/batch_size)                #number of batches derived from batch_size\n",
    "    indices = np.random.permutation(m)             # generate ramdom indicies\n",
    "    shuffle_X = X[:,indices]\n",
    "    shuffle_Y = Y[:,indices]\n",
    "    mini_batches = []\n",
    "    \n",
    "    #generate minibatch\n",
    "    for i in range(num_batches):\n",
    "        ##Start code here\n",
    "        X_batch = \n",
    "        Y_batch = \n",
    "        ##End code\n",
    "        \n",
    "        assert X_batch.shape == (X.shape[0], batch_size)\n",
    "        assert Y_batch.shape == (Y.shape[0], batch_size)\n",
    "        \n",
    "        mini_batches.append((X_batch, Y_batch))\n",
    "    \n",
    "    #generate batch with remaining number of samples\n",
    "    if m % batch_size != 0:\n",
    "        ##Srart code here\n",
    "        X_batch = \n",
    "        Y_batch = \n",
    "        ##Srart code here\n",
    "        mini_batches.append((X_batch, Y_batch))\n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model to train the network using minibatch\n",
    "- parameters:\n",
    "  - X_train, Y_train: input and target data\n",
    "  - layer_dims: network configuration\n",
    "  - learning_rate\n",
    "  - num_iter: number of epoches\n",
    "  - mini_batch_size: number of samples to be considered in each minibatch\n",
    "- return: dictionary of trained parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_with_minibatch(X_train,Y_train, layer_dims, learning_rate, num_iter, mini_batch_size):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    num_features, num_samples = X_train.shape\n",
    "    num_classes = Y_train.shape[0]\n",
    "    A_0, Y = placeholders(num_features, num_classes)\n",
    "    parameters = initialize_parameters_deep(layer_dims)\n",
    "    Z_final = l_layer_forwardProp(A_0, parameters)\n",
    "    cost = final_cost(Z_final, Y)\n",
    "    train_net = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "    seed = 1\n",
    "    num_minibatches = int(num_samples / mini_batch_size)\n",
    "    init = tf.global_variables_initializer()\n",
    "    costs = []\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for epoch in range(num_iter):\n",
    "            epoch_cost = 0\n",
    "            mini_batches = random_samples_minibatch(X_train, Y_train, mini_batch_size, seed) #generate array of minibatch using random_samples_minibatch()\n",
    "            seed = seed + 1\n",
    "            \n",
    "            #perform gradient descent for each mini-batch\n",
    "            for mini_batch in mini_batches: \n",
    "                ##Start code\n",
    "                X_batch, Y_batch = \n",
    "                _,mini_batch_cost = \n",
    "                ##End code\n",
    "                \n",
    "                epoch_cost += mini_batch_cost/num_minibatches\n",
    "            if epoch % 1 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "            if epoch % 10 == 0:\n",
    "                print(epoch_cost)\n",
    "        with open('output.txt', 'w') as file:\n",
    "            file.write(\"cost = %f\" % costs[-1])\n",
    "        plt.ylim(0, max(costs), 0.0001)\n",
    "        plt.xlabel(\"epoches per 100\")\n",
    "        plt.ylabel(\"cost\")\n",
    "        plt.plot(costs)\n",
    "        plt.show()\n",
    "        params = sess.run(parameters)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = model_with_minibatch(X.T, Y, layer_dims, 0.01, 100, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The function to predict the ouput has been defined for you.  \n",
    "- The function takes input and trained parameters and predict the output using forward propagation  \n",
    "- the arg_max function returns the index of largest probability value in out y of each sample  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(X_train, params):\n",
    "    with tf.Session() as sess:\n",
    "        Y = tf.arg_max(l_layer_forwardProp(X_train,params), dimension=1)\n",
    "        return sess.run(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next two cell to visualize the boundary predicted by the model (This takes slightly longer time for the plot to appear)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_decision_boundary1( X, y, model):\n",
    "    plt.clf()\n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1\n",
    "    y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1   \n",
    "    colors=['blue','green','red', 'yellow']\n",
    "    cmap = matplotlib.colors.ListedColormap(colors)   \n",
    "    h = 0.01\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    # Predict the function value for the whole grid\n",
    "    A = model(np.c_[xx.ravel(), yy.ravel()])\n",
    "    A = A.reshape(xx.shape)\n",
    "    # Plot the contour and training examples\n",
    "    plt.contourf(xx, yy, A, cmap=\"spring\")\n",
    "    plt.ylabel('x2')\n",
    "    plt.xlabel('x1')\n",
    "    plt.scatter(X[0, :], X[1, :], c=y, s=8,cmap=cmap)\n",
    "    plt.title(\"Decision Boundary\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_decision_boundary1(X_train,Y_train,lambda x: predict(x.T,params))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
